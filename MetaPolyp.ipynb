{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhplinh/Polyp_Segmentation/blob/main/MetaPolyp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wywQ2XqsF1Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63206942-6dab-460e-a5a9-0d5f948af0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.9 -y\n",
        "!echo 2 | sudo update-alternatives --config python3"
      ],
      "metadata": {
        "id": "LhimcWzNc_X6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6434892-dfe0-40a3-c890-09ea33d12f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [805 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,120 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,687 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,969 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [61.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,166 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.7 kB]\n",
            "Fetched 10.5 MB in 2s (5,587 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.9-minimal libpython3.9-stdlib mailcap mime-support\n",
            "  python3.9-minimal\n",
            "Suggested packages:\n",
            "  python3.9-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.9-minimal libpython3.9-stdlib mailcap mime-support python3.9\n",
            "  python3.9-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 5,275 kB of archives.\n",
            "After this operation, 19.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-minimal amd64 3.9.19-1+jammy1 [835 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-minimal amd64 3.9.19-1+jammy1 [2,073 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-stdlib amd64 3.9.19-1+jammy1 [1,842 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9 amd64 3.9.19-1+jammy1 [497 kB]\n",
            "Fetched 5,275 kB in 1s (4,336 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.9-minimal:amd64.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.9-minimal_3.9.19-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-minimal:amd64 (3.9.19-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-minimal.\n",
            "Preparing to unpack .../1-python3.9-minimal_3.9.19-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9-minimal (3.9.19-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.9-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.9-stdlib_3.9.19-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-stdlib:amd64 (3.9.19-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9.\n",
            "Preparing to unpack .../5-python3.9_3.9.19-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9 (3.9.19-1+jammy1) ...\n",
            "Setting up libpython3.9-minimal:amd64 (3.9.19-1+jammy1) ...\n",
            "Setting up python3.9-minimal (3.9.19-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.9-stdlib:amd64 (3.9.19-1+jammy1) ...\n",
            "Setting up python3.9 (3.9.19-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "update-alternatives: error: no alternatives for python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/MetaPolyp - Polyp Segmentation/MetaPolyp-CBMS2023\n",
        "\n",
        "# %cd /content/drive/MyDrive/Colab Notebooks/Colab Notebooks/MetaPolyp - Polyp Segmentation/MetaPolyp-CBMS2023"
      ],
      "metadata": {
        "id": "JVkXzIT1Lwme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c9980d-e00b-4808-c333-ba34f9f054b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/MetaPolyp - Polyp Segmentation/MetaPolyp-CBMS2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fnBhHT5ASIr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "yfCqawHVO0oC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c003f6bd-43f8-4f4e-9abf-0b874ffa4a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.8.0.76)\n",
            "Collecting tensorflow==2.11.0 (from -r requirements.txt (line 2))\n",
            "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.7.1)\n",
            "Collecting keras-cv-attention-models==1.3.9 (from -r requirements.txt (line 8))\n",
            "  Downloading keras_cv_attention_models-1.3.9-py3-none-any.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.5/572.5 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (3.9.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (24.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 2)) (0.36.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (4.9.4)\n",
            "Collecting tensorflow-addons (from keras-cv-attention-models==1.3.9->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0->-r requirements.txt (line 2)) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (3.0.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (2.3)\n",
            "INFO: pip is looking at multiple versions of tensorflow-datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-datasets (from keras-cv-attention-models==1.3.9->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (0.5.1)\n",
            "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_datasets-4.9.1-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_datasets-4.9.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (0.10.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (3.18.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8)) (1.63.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets->keras-cv-attention-models==1.3.9->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
            "  Downloading tensorflow_metadata-1.13.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, typeguard, tensorflow-estimator, tensorboard-data-server, protobuf, keras, gast, tensorflow-addons, tensorflow-metadata, google-auth-oauthlib, tensorflow-datasets, tensorboard, tensorflow, keras-cv-attention-models\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: tensorflow-metadata\n",
            "    Found existing installation: tensorflow-metadata 1.14.0\n",
            "    Uninstalling tensorflow-metadata-1.14.0:\n",
            "      Successfully uninstalled tensorflow-metadata-1.14.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.9.4\n",
            "    Uninstalling tensorflow-datasets-4.9.4:\n",
            "      Successfully uninstalled tensorflow-datasets-4.9.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.11.0 keras-cv-attention-models-1.3.9 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-addons-0.23.0 tensorflow-datasets-4.9.0 tensorflow-estimator-2.11.0 tensorflow-metadata-1.13.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "023ce289e52f45c896d859da230f0437"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from model import build_model\n",
        "import numpy as np\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = build_model()\n",
        "    model.load_weights(model_path)\n",
        "    return model\n",
        "\n",
        "def predict_single(model, imgPath):\n",
        "    img = cv2.imread(imgPath)\n",
        "    img = cv2.resize(img, (256, 256))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = np.expand_dims(img, axis = 0)\n",
        "    # img = img / 255.\n",
        "    result = model.predict(img)\n",
        "    return result\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.squeeze(mask)\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask*255\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_path = \"pretrained_model.h5\"\n",
        "    img_in = \"0small.jpg\"\n",
        "    img_out = \"0smallout.jpg\"\n",
        "\n",
        "    model = load_model(save_path)\n",
        "    masks = predict_single(model, img_in)\n",
        "    out = mask_parse(masks)\n",
        "\n",
        "    cv2.imwrite(img_out, out)\n",
        "\n",
        "\n",
        "    # img_in = \"image (571).jpg\"\n",
        "    # img_out = \"imageout571.jpg\"\n",
        "    # masks = predict_single(model, img_in)\n",
        "    # out = mask_parse(masks)\n",
        "    # cv2.imwrite(img_out, out)\n",
        "\n",
        "\n",
        "\n",
        "    # img_in = \"image (820).jpg\"\n",
        "    # img_out = \"imageout820.jpg\"\n",
        "    # masks = predict_single(model, img_in)\n",
        "    # out = mask_parse(masks)\n",
        "    # cv2.imwrite(img_out, out)\n"
      ],
      "metadata": {
        "id": "Avr6ciHn3j4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d4df21-2f2f-4bea-d6cb-cda9d6084f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> Load pretrained from: /root/.keras/models/caformer_s18_224_imagenet.h5\n",
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from save_model.pvt_CAM_channel_att_upscale import build_model\n",
        "import os\n",
        "import tensorflow as tf\n",
        "# from metrics.metrics_last import  iou_metric, MAE, WFbetaMetric, SMeasure, Emeasure,  dice_coef, iou_metric\n",
        "from metrics.segmentation_metrics import dice_coeff, bce_dice_loss, IoU, zero_IoU, dice_loss\n",
        "from dataloader.dataloader import build_augmenter, build_dataset, build_decoder\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from model import build_model\n",
        "\n",
        "from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "def load_dataset(route):\n",
        "    X_path = '{}/images/'.format(route)\n",
        "    Y_path = '{}/masks/'.format(route)\n",
        "    X_full = sorted(os.listdir(f'{route}/images'))\n",
        "    Y_full = sorted(os.listdir(f'{route}/masks'))\n",
        "\n",
        "    X_train = [X_path + x for x in X_full]\n",
        "    Y_train = [Y_path + x for x in Y_full]\n",
        "\n",
        "    test_decoder = build_decoder(with_labels=False, target_size=(img_size, img_size), ext='jpg',\n",
        "                                segment=True, ext2='jpg')\n",
        "    test_dataset = build_dataset(X_train, Y_train, bsize=BATCH_SIZE, decode_fn=test_decoder,\n",
        "                                augmentAdv=False, augment=False, augmentAdvSeg=False)\n",
        "    return test_dataset, len(X_train)\n",
        "    # X_full is name of images for benchmarking\n",
        "    # return test_dataset, len(X_train), X_full\n",
        "\n",
        "precision = Precision()\n",
        "recall = Recall()\n",
        "\n",
        "@tf.function\n",
        "def F1Score(y_true, y_pred):\n",
        "    precision_score = precision(y_true, y_pred)\n",
        "    recall_score = recall(y_true, y_pred)\n",
        "    return 2 * ((precision_score * recall_score) / (precision_score + recall_score + 1e-7))\n",
        "\n",
        "def benchmark(route, model, BATCH_SIZE = 32, save_file_name = \"benchmark_result.txt\"):\n",
        "    list_of_datasets = os.listdir(route)\n",
        "    f = open(save_file_name,\"a\")\n",
        "    f.write(\"\\n\")\n",
        "    for datasets in list_of_datasets:\n",
        "        print(datasets, \":\")\n",
        "        test_dataset, len_data = load_dataset(os.path.join(route,datasets))\n",
        "        steps_per_epoch = len_data // BATCH_SIZE\n",
        "        loss, dice_coeff, bce_dice_loss, IoU, zero_IoU, mae, precision, recall, accuracy, f1_score = model.evaluate(test_dataset, steps=steps_per_epoch)\n",
        "#         loss, dice_coeff, bce_dice_loss, IoU, zero_IoU, mae = model.evaluate(test_dataset, steps=steps_per_epoch)\n",
        "        f.write(\"{}:\".format(datasets))\n",
        "        f.write(\"dice_coeff: {}, bce_didce_loss: {}, IoU: {}, zero_IoU: {}, mae: {}, precision: {}, recall: {}, accuracy: {}, f1_score: {}\".format(dice_coeff, bce_dice_loss, IoU, zero_IoU, mae, precision, recall, accuracy, f1_score))\n",
        "#        f.write(\"dice_coeff: {}, bce_didce_loss: {}, IoU: {}, zero_IoU: {}, mae: {}\".format(dice_coeff, bce_dice_loss, IoU, zero_IoU, mae))\n",
        "        f.write('\\n')\n",
        "\n",
        "\n",
        "# def benchmark(route, model, BATCH_SIZE = 1, save_file_name = \"benchmark_result.csv\"):\n",
        "#     list_of_datasets = os.listdir(route)\n",
        "#     with open(save_file_name, mode='w', newline='') as file:\n",
        "#         writer = csv.writer(file)\n",
        "#         writer.writerow([\"Dataset\", \"Image Index\", \"Image Name\", \"Dice Coefficient\", \"mIoU\"])\n",
        "#         for datasets in list_of_datasets:\n",
        "#             print(datasets, \":\")\n",
        "#             test_dataset, len_data, image_names = load_dataset(os.path.join(route,datasets))\n",
        "#             for i, (images, masks) in enumerate(test_dataset):\n",
        "#                 if i >= len_data:\n",
        "#                     break\n",
        "#                 print(f\"Processing image {i+1} of {len_data}: {image_names[i]}\")\n",
        "#                 preds = model.predict(images, verbose=0)\n",
        "#                 dice_coeff_value = dice_coeff(masks, preds).numpy()\n",
        "#                 mIoU = np.mean(IoU(masks, preds).numpy())\n",
        "#                 writer.writerow([datasets, i, image_names[i], dice_coeff_value, mIoU])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    img_size = 256\n",
        "    BATCH_SIZE = 1\n",
        "    SEED = 1024\n",
        "\n",
        "    #change save_path name to load model\n",
        "    save_path = \"pretrained_model.h5\"\n",
        "    # route_data = \"./TestDataset/\"\n",
        "    # route_data = \"/content/drive/MyDrive/hcmus/HCMUS courses/Year 2 Sem 2/Technical Writing/Computer Vision - Polyp Segmentation/Datasets\"\n",
        "\n",
        "    path_to_test_dataset = route_data\n",
        "    model = build_model(img_size)\n",
        "    model.load_weights(save_path)\n",
        "\n",
        "    # model.compile(metrics=[dice_coeff, bce_dice_loss, IoU, zero_IoU, tf.keras.metrics.MeanSquaredError()])\n",
        "    model.compile(metrics=[dice_coeff, bce_dice_loss, IoU, zero_IoU, tf.keras.metrics.MeanSquaredError(), Precision(), Recall(), Accuracy(), F1Score])\n",
        "    # model.compile(metrics=[dice_coeff, IoU])\n",
        "\n",
        "    benchmark(path_to_test_dataset, model)"
      ],
      "metadata": {
        "id": "JxVql63fR4O1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "e6407ced-4829-4350-9db3-f75cf5a420b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> Load pretrained from: /root/.keras/models/caformer_s18_224_imagenet.h5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Datasets/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-92bb6e2423d9>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# model.compile(metrics=[dice_coeff, IoU])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-92bb6e2423d9>\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(route, model, BATCH_SIZE, save_file_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"benchmark_result.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mlist_of_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Datasets/'"
          ]
        }
      ]
    }
  ]
}